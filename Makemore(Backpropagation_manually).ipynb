{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1fk5yG4tKRM0qVz2VyU9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrish23/Makemore/blob/master/Makemore(Backpropagation_manually).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IzUEaH9pQfNZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5TDxR2pXdAS",
        "outputId": "8c1438f3-e866-4c73-c936-be5cb163de59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNdLnPEhX-kJ",
        "outputId": "f170f2fe-5551-4d75-8afe-65e6906f0a50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3# context length: how many characters do we take to predict the next one\n",
        "\n",
        "def build_dataset(words):\n",
        "  X,Y = [],[]\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix]  #crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X,Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])\n",
        "Xte, Yte = build_dataset(words[n2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx8hVl1JYVYZ",
        "outputId": "c1947735-4d0f-4f04-b3bb-2fe1e4602b86"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182580, 3]) torch.Size([182580])\n",
            "torch.Size([22767, 3]) torch.Size([22767])\n",
            "torch.Size([22799, 3]) torch.Size([22799])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later to compare manual to PyTorch gradients\n",
        "def cmp(s,dt,t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt,t.grad)# checking if the two values differ only by some floating numbers then it approximates them\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "mp0dQm9mg2by"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_emd = 10# the dimensionality of the character embedding vectors\n",
        "n_hidden = 64# the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)# for reproducibility\n",
        "C = torch.randn((vocab_size, n_emd), generator=g)\n",
        "#Layer 1\n",
        "W1 = torch.randn((n_emd*block_size, n_hidden), generator=g) *(5/3)/((n_emd*block_size)**0.5) #* 0.2\n",
        "b1 = torch.randn(n_hidden,                     generator=g) * 0.1 #using for fun, it's useless\n",
        "#Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),       generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                   generator=g) * 0.1\n",
        "\n",
        "#batch normalization parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "#Note: We are initializing many of thrsr paramters in non-standard ways\n",
        "# because sometimes initializing with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass\n",
        "\n",
        "parameters = [C, W1, b1, W2,b2,bngain,bnbias]\n",
        "\n",
        "print(sum(p.nelement() for p in parameters))#number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1mtZn--hNKv",
        "outputId": "e8dac4d7-8b6f-46c9-8dce-767f4c2f00d4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,),generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix]# batch X, Y"
      ],
      "metadata": {
        "id": "5gjjEGy8kUye"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuaBViTZmKe7",
        "outputId": "9ef568a6-6bfa-48e3-cd9e-ca85dd3e397b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.5069, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts.shape, counts_sum.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-RYKuucbh8E",
        "outputId": "20e533dc-ae53-4f05-be40-635b49f883fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_logits.shape, logits.shape, logit_maxes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7G7lKiSPqZh",
        "outputId": "bb59aecf-7fbe-4368-ca8c-638554543ba6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "RIFKf-0WptZF",
        "outputId": "e2ccbf9b-15fd-4a91-d578-5250ca07bbf6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7802fac15c00>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE can determine the derivatives by looking at the shapes of these matrices."
      ],
      "metadata": {
        "id": "V1YqTgdVU2ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits.shape, h.shape, W2.shape, b2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkvWHIZvUfwi",
        "outputId": "820a4841-d0a2-49ad-f4dc-31008106a404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([64, 27]),\n",
              " torch.Size([27]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the shape of h is 32,64 so we need that shape. The derivative of h will contain dlogits and W2, so looking at the shapes of the matrices we can write the derivatives of h and W2 as follows"
      ],
      "metadata": {
        "id": "E5Guiu_XVfJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)"
      ],
      "metadata": {
        "id": "GtwOO6hlh9kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGqmvMFmo8Zu",
        "outputId": "828fbdb7-e469-4f9c-979c-8356bbd4a213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([1, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnraw.shape, bndiff.shape, bnvar_inv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79-aRJsrf_L",
        "outputId": "fcca84f5-22fe-4e3e-c2fe-0b9073dfdc83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnvar.shape, bndiff2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rSU62H66u_C",
        "outputId": "019bb9ae-95d7-4415-be22-99b04db60e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64]), torch.Size([32, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnvar.shape, bndiff2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIEfDKZBtmTs",
        "outputId": "1ca97849-d7e5-4437-9934-2f49f90cc2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64]), torch.Size([32, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# loss = -(a + b + c)/3 = (-1/3)a + (-1/3)b + (-1/3)c\n",
        "# dloss/da = -1/3 = -1/n\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n),Yb] = -1.0/n\n",
        "\n",
        "dprobs = (1.0 / probs) * dlogprobs#chaining\n",
        "\n",
        "#c = a * b but with tensors\n",
        "# a[3x3] * b[3x1] --->\n",
        "# a11*b1 a12*b1 a13*b1\n",
        "# a21*b2 a22*b2 a23*b2\n",
        "# a31*b3 a32*b3 a33*b3\n",
        "# since due to broadcasting the b tensor is replicated so when derivating the derivative is summed accros columns as well\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)#chain rule\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "\n",
        "dnorm_logits = counts * dcounts #because the derivative of norm_logits.exp() is the same\n",
        "\n",
        "dlogits = dnorm_logits.clone()\n",
        "\n",
        "dlogit_maxes = (-dnorm_logits).sum(1,keepdims=True)\n",
        "\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - (h**2)) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdims=True)\n",
        "\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0,keepdims=True)\n",
        "\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "\n",
        "dhprebn = dbndiff.clone()\n",
        "\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "\n",
        "dhprebn += (1.0/n)*torch.ones_like(hprebn) * dbnmeani\n",
        "\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "\n",
        "demb = dembcat.view(emb.shape) # As both are same but just the dimensions are different\n",
        "\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-frck1Ejpelx",
        "outputId": "57a78a14-5861-44b5-9898-96547d364d85"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 5.238689482212067e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 1.6370904631912708e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 3.958120942115784e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyNLoRpaYLDJ",
        "outputId": "04e8706c-14ac-47dd-a224-54695ba07f8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.316056251525879 diff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward Pass\n",
        "\n",
        "dlogits = F.softmax(logits,1)\n",
        "dlogits[range(n),Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmHQC-8HPo-Q",
        "outputId": "b4d3824b-5126-47cb-99b3-01ddb4132071"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 6.752088665962219e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI45SN8fMKZz",
        "outputId": "13cedf59-9f02-4e67-aadd-01224890eb30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0707,  0.0872,  0.0187,  0.0476,  0.0178,  0.0794,  0.0239,  0.0383,\n",
              "        -0.9796,  0.0322,  0.0343,  0.0371,  0.0353,  0.0286,  0.0360,  0.0136,\n",
              "         0.0094,  0.0200,  0.0161,  0.0535,  0.0549,  0.0218,  0.0238,  0.0733,\n",
              "         0.0580,  0.0259,  0.0221], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dlogits are the probabilities of each character"
      ],
      "metadata": {
        "id": "70OWx2zDNR4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "3ePyU9iyFD6u",
        "outputId": "4f8bff60-9be5-4aa4-b83d-c0a610e883a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a11a5818ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYrlNd-fMIVU",
        "outputId": "7681438d-4d22-4fb5-c09d-e90a491cfe60"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0)) # TODO. my solution is 1 (long) line\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "id": "sXk3ReHOscLT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e3cc04-5ce5-430c-c51f-1faf7935180e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "#with torch.no_grad():\n",
        "\n",
        "# kick off optimization\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmean = hprebn.mean(0, keepdim=True)\n",
        "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "  hpreact = bngain * bnraw + bnbias\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "  # manual backprop! #swole_doge_meme\n",
        "  # -----------------\n",
        "  dlogits = F.softmax(logits,1)\n",
        "  dlogits[range(n),Yb] -= 1\n",
        "  dlogits /= n\n",
        "  #2nd Layer backpropogation\n",
        "  dh = dlogits @ W2.T\n",
        "  dW2 = h.T @ dlogits\n",
        "  db2 = dlogits.sum(0)\n",
        "  #tanh\n",
        "  dhpreact = (1.0 - (h**2)) * dh\n",
        "  #Batchnorm Backpropogation\n",
        "  dbngain = (bnraw * dhpreact).sum(0, keepdims=True)\n",
        "  dbnbias = dhpreact.sum(0, keepdims=True)\n",
        "  dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
        "  # 1st Layer\n",
        "  dembcat = dhprebn @ W1.T\n",
        "  dW1 = embcat.T @ dhprebn\n",
        "  db1 = dhprebn.sum(0)\n",
        "  # embedding\n",
        "  demb = dembcat.view(emb.shape)\n",
        "  dC = torch.zeros_like(C)\n",
        "  for k in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "      ix = Xb[k,j]\n",
        "      dC[ix] += demb[k,j]\n",
        "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "  # -----------------\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p, grad in zip(parameters, grads):\n",
        "    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "    #p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "  #   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIxM2YXW_VHV",
        "outputId": "bf8a9e4e-49d7-4339-bebc-b5b56dbd236c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.6146\n",
            "  10000/ 200000: 2.4684\n",
            "  20000/ 200000: 2.3442\n",
            "  30000/ 200000: 2.1333\n",
            "  40000/ 200000: 2.0003\n",
            "  50000/ 200000: 2.4140\n",
            "  60000/ 200000: 2.2570\n",
            "  70000/ 200000: 2.1232\n",
            "  80000/ 200000: 1.9372\n",
            "  90000/ 200000: 2.0809\n",
            " 100000/ 200000: 2.3961\n",
            " 110000/ 200000: 2.1975\n",
            " 120000/ 200000: 2.1035\n",
            " 130000/ 200000: 2.3433\n",
            " 140000/ 200000: 2.3076\n",
            " 150000/ 200000: 2.3112\n",
            " 160000/ 200000: 2.0767\n",
            " 170000/ 200000: 1.9984\n",
            " 180000/ 200000: 2.3137\n",
            " 190000/ 200000: 1.9236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMomv9sVZext",
        "outputId": "64bbcc5e-f421-4e0f-a8da-baf5743975b8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 1.6763806343078613e-08\n",
            "(200,)          | exact: False | approximate: False | maxdiff: 1.1175870895385742e-08\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 2.2351741790771484e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "byHtGqjwZioI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eG9zKWjZl5F",
        "outputId": "03cd58d0-bcc5-4b45-912e-f3ff45640a1b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0717644691467285\n",
            "val 2.1136083602905273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSL68FacZne3",
        "outputId": "77b3866e-93eb-4b81-d257-0abe521e2a84"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora.\n",
            "kayanniee.\n",
            "med.\n",
            "ryla.\n",
            "remmadiendra.\n",
            "gradelynnelin.\n",
            "shi.\n",
            "jen.\n",
            "eden.\n",
            "sananarielleigh.\n",
            "kalin.\n",
            "shubergihimiel.\n",
            "kindreelynn.\n",
            "novana.\n",
            "ubreyce.\n",
            "ryyshul.\n",
            "eli.\n",
            "kayshuston.\n",
            "mahil.\n",
            "samyansun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dZh6xgqEgT6J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}